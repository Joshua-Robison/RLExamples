{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "Imagine a robot navigating the maze shown below:\n",
    "\n",
    "![maze](../references/maze.png)\n",
    "\n",
    "It starts at point <b>E</b> (Entrance) and wants to get to point <b>G</b> (Goal). We are going to build an AI that can find the quickest way to the Goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set randomization for reproducing results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to define the environment with the following locations and states shown below:\n",
    "\n",
    "![states](../references/states.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the robot can perform specific actions for each location it is in at time t. This can be represented by an adjacency matrix, where +1 indicates a reward for an action it can perform and 0 indicates a reward for an action it cannot perform.\n",
    "\n",
    "However, to incentivize our AI to achieve its goal (going to G), we need to place a <b>higher additional reward</b> at location (G,G) in our adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the possible actions\n",
    "actions = np.array(range(0,12))\n",
    "\n",
    "# define the possible states\n",
    "states = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11}\n",
    "\n",
    "# make a mapping from states to locations\n",
    "locations = {state: location for location, state in states.items()}\n",
    "\n",
    "# define the rewards matrix\n",
    "#                    A,B,C,D,E,F,G,H,I,J,K,L\n",
    "rewards = np.array([[0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "                    [1,0,1,0,0,1,0,0,0,0,0,0],\n",
    "                    [0,1,0,0,0,0,1,0,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "                    [0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "                    [0,1,0,0,0,0,0,0,0,1,0,0],\n",
    "                    [0,0,1,0,0,0,0,1,0,0,0,0],\n",
    "                    [0,0,0,1,0,0,1,0,0,0,0,1],\n",
    "                    [0,0,0,0,1,0,0,0,0,1,0,0],\n",
    "                    [0,0,0,0,0,1,0,0,1,0,1,0],\n",
    "                    [0,0,0,0,0,0,0,0,0,1,0,1],\n",
    "                    [0,0,0,0,0,0,0,1,0,0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the gamma and alpha parameters for Q-Learning\n",
    "γ = 0.75\n",
    "α = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "N = 1_000\n",
    "\n",
    "# initialize the Q-values\n",
    "Q = np.zeros((12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the goal location a high reward\n",
    "goal = 'G'\n",
    "ix = states[goal]\n",
    "rewards[ix, ix] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the Q-Learning process\n",
    "options = list(states.values())\n",
    "for i in range(N):\n",
    "    # pick random current state\n",
    "    current_state = np.random.choice(options)\n",
    "\n",
    "    # get available moves based on current state\n",
    "    playable_actions = []\n",
    "    for j in range(rewards.shape[1]):\n",
    "        if rewards[current_state,j] > 0:\n",
    "            playable_actions.append(j) # add to list of options\n",
    "\n",
    "    # pick a move\n",
    "    next_state = np.random.choice(playable_actions)\n",
    "\n",
    "    # compute the temporal difference\n",
    "    TD = rewards[current_state, next_state] + γ * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n",
    "\n",
    "    # update the Q-value\n",
    "    Q[current_state, next_state] = Q[current_state, next_state] + α * TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "[[   0 1684    0    0    0    0    0    0    0    0    0    0]\n",
      " [1259    0 2245    0    0 1259    0    0    0    0    0    0]\n",
      " [   0 1683    0    0    0    0 2992    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0 2245    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0  711    0    0    0]\n",
      " [   0 1678    0    0    0    0    0    0    0  947    0    0]\n",
      " [   0    0 2244    0    0    0 3988 2244    0    0    0    0]\n",
      " [   0    0    0 1684    0    0 2992    0    0    0    0 1684]\n",
      " [   0    0    0    0  534    0    0    0    0  947    0    0]\n",
      " [   0    0    0    0    0 1259    0    0  709    0 1262    0]\n",
      " [   0    0    0    0    0    0    0    0    0  947    0 1681]\n",
      " [   0    0    0    0    0    0    0 2245    0    0 1261    0]]\n"
     ]
    }
   ],
   "source": [
    "# display the final Q-values\n",
    "print('Q-values:\\n{}'.format(Q.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return optimally computed route\n",
    "def route(starting_location, ending_location):\n",
    "    # create a list for the optimal route\n",
    "    route = [starting_location] # add the starting point to the route\n",
    "    \n",
    "    # set the next location to the current location\n",
    "    next_location = starting_location\n",
    "    \n",
    "    # keep running until we reach the end location\n",
    "    while (next_location != ending_location):\n",
    "        # get the value associated with the current location\n",
    "        starting_state = states[starting_location]\n",
    "        \n",
    "        # use that value to get the best Q-value index\n",
    "        next_state = np.argmax(Q[starting_state,])\n",
    "        \n",
    "        # index that Q-value on our dictionary of locations\n",
    "        next_location = locations[next_state]\n",
    "        \n",
    "        # add the location to our list of steps\n",
    "        route.append(next_location)\n",
    "        \n",
    "        # update the current state for next iteration\n",
    "        starting_location = next_location\n",
    "        \n",
    "    return route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal route:\n",
      "\n",
      "['E', 'I', 'J', 'K', 'L', 'H', 'G']\n"
     ]
    }
   ],
   "source": [
    "# print the results\n",
    "print('Optimal route:\\n\\n{}'.format(route('E', goal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to solve a harder problem. Imagine a warehouse maze as shown below:\n",
    "\n",
    "![warehouse](../references/warehouse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, black squares are shelves. The robot can run into a shelf, so the goal is to navigate the aisles and get to the green square. This time, we will not be restricting the robot's possible actions. Instead we will teach it to learn not to run into shelves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment\n",
    "environment_rows = 11\n",
    "environment_cols = 11\n",
    "\n",
    "# initialize the Q-values\n",
    "Q = np.zeros((environment_rows, environment_cols, 4)) # [state, state, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actions\n",
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the rewards. For this example, we want to get to the goal. If we made all white squares +1 and all black squares 0 or -1, the AI agent <b>could</b> maximize its reward by moving along the white squares forever before reaching its destination. Instead we want to incentivize our AI agent to move efficiently. We can do this with the rewards matrix defined below:\n",
    "\n",
    "![rewards](../references/warehouse_rewards.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the rewards\n",
    "rewards = np.full((environment_rows, environment_cols), -100.0)\n",
    "rewards[0, 5] = 100.0 # goal location\n",
    "\n",
    "# define aisle locations\n",
    "aisles = {}\n",
    "aisles[1] = [i for i in range(1, 10)]\n",
    "aisles[2] = [1, 7, 9]\n",
    "aisles[3] = [i for i in range(1, 8)]\n",
    "aisles[3].append(9)\n",
    "aisles[4] = [3, 7]\n",
    "aisles[5] = [i for i in range(0, 11)]\n",
    "aisles[6] = [5]\n",
    "aisles[7] = [i for i in range(1, 10)]\n",
    "aisles[8] = [3, 7]\n",
    "aisles[9] = [i for i in range(0, 11)]\n",
    "\n",
    "# set aisle rewards\n",
    "for row in range(1, 10):\n",
    "    for col in aisles[row]:\n",
    "        rewards[row, col] = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
      "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
      "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
     ]
    }
   ],
   "source": [
    "# view rewards matrix\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper functions\n",
    "def is_terminal_state(row, col):\n",
    "    \"\"\"This function returns whether or not we are at a terminal state.\"\"\"\n",
    "    return not rewards[row, col] == -1.0\n",
    "\n",
    "def get_starting_location():\n",
    "    \"\"\"This function initializes a random non-terminal starting location.\"\"\"\n",
    "    row = np.random.randint(environment_rows) # random row\n",
    "    col = np.random.randint(environment_cols) # random col\n",
    "    # keep choosing until a non-terminal state is identified\n",
    "    # (i.e. a white aisle square)\n",
    "    while is_terminal_state(row, col):\n",
    "        row = np.random.randint(environment_rows)\n",
    "        col = np.random.randint(environment_cols)\n",
    "        \n",
    "    return row, col\n",
    "\n",
    "def get_next_action(row, col, ϵ):\n",
    "    \"\"\"This function uses an epsilon greedy algorithm to choose the next action.\"\"\"\n",
    "    if np.random.random() < ϵ:\n",
    "        # choose most promising option\n",
    "        return np.argmax(Q[row, col])\n",
    "    else:\n",
    "        # choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "def get_next_location(row, col, action):\n",
    "    \"\"\"This function gets the next location based on the chosen action.\"\"\"\n",
    "    new_row, new_col = row, col\n",
    "    if actions[action] == 'up' and row > 0:\n",
    "        new_row -= 1\n",
    "    elif actions[action] == 'down' and row < environment_rows - 1:\n",
    "        new_row += 1\n",
    "    elif actions[action] == 'left' and col > 0:\n",
    "        new_col -= 1\n",
    "    elif actions[action] == 'right' and col < environment_cols - 1:\n",
    "        new_col += 1\n",
    "    \n",
    "    return new_row, new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define shortest path function\n",
    "def get_shortest_path(start_row, start_col):\n",
    "    \"\"\"This function returns the shortest path from the starting state to the goal.\"\"\"\n",
    "    if is_terminal_state(start_row, start_col):\n",
    "        return [] # invalid starting location\n",
    "    else:\n",
    "        # save current state\n",
    "        current_row, current_col = start_row, start_col\n",
    "        \n",
    "        # make best path list and add current state\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row, current_col])\n",
    "        \n",
    "        # continue moving until the goal is reached\n",
    "        while not is_terminal_state(current_row, current_col):\n",
    "            # get best action to take\n",
    "            action = get_next_action(current_row, current_col, 1.)\n",
    "            \n",
    "            # update the state\n",
    "            current_row, current_col = get_next_location(current_row, current_col, action)\n",
    "            \n",
    "            # update path\n",
    "            shortest_path.append([current_row, current_col])\n",
    "        \n",
    "        return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 9], [2, 9], [1, 9], [0, 9]]\n",
      "[[5, 0], [4, 0]]\n",
      "[[9, 5], [8, 5]]\n"
     ]
    }
   ],
   "source": [
    "# display untrained answers\n",
    "print(get_shortest_path(3, 9)) # row 3, col 9\n",
    "print(get_shortest_path(5, 0)) # row 5, col 0\n",
    "print(get_shortest_path(9, 5)) # row 9, col 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# train AI agent using Q-Learning\n",
    "# define training parameters\n",
    "ϵ = 0.90 # greedy epsilon algorithm\n",
    "γ = 0.90 # discount factor\n",
    "α = 0.90 # learning rate\n",
    "\n",
    "# number of iterations\n",
    "N = 1_000\n",
    "\n",
    "# simulate\n",
    "for i in range(N):\n",
    "    # get starting location\n",
    "    row, col = get_starting_location()\n",
    "    \n",
    "    # continue moving until we reach a terminal state\n",
    "    # (reach the goal or crash into an aisle)\n",
    "    while not is_terminal_state(row, col):\n",
    "        # get next action to take\n",
    "        action = get_next_action(row, col, ϵ)\n",
    "        \n",
    "        # perform action and transition to next state\n",
    "        old_row, old_col = row, col\n",
    "        row, col = get_next_location(row, col, action)\n",
    "        \n",
    "        # receive reward for moving to new state\n",
    "        reward = rewards[row, col]\n",
    "        old_Q = Q[old_row, old_col, action]\n",
    "        \n",
    "        # compute temporal difference\n",
    "        TD = reward + γ * np.max(Q[row, col]) - old_Q\n",
    "        \n",
    "        # update Q-value for previous state and action pair\n",
    "        new_Q = old_Q + α * TD\n",
    "        Q[old_row, old_col, action] = new_Q\n",
    "        \n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values:\n",
      "[[[   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [ -99   -3  -99   62]\n",
      "  [ -99  -99   54   70]\n",
      "  [ -99  -99   62   79]\n",
      "  [ -99  -99   70   89]\n",
      "  [ 100 -100   79   79]\n",
      "  [ -99  -99   89   70]\n",
      "  [-100   62   79   62]\n",
      "  [ -99  -90   70   54]\n",
      "  [ -99   43   62  -99]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [  54   37  -99  -90]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  70   54  -99  -99]\n",
      "  [   0    0    0    0]\n",
      "  [  54   -2  -90  -99]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [  48  -90  -99   -4]\n",
      "  [ -90  -99   42   -5]\n",
      "  [ -90   -5   -5   37]\n",
      "  [ -99  -99   -4   42]\n",
      "  [ -99  -99   33   48]\n",
      "  [ -90  -99   42   54]\n",
      "  [  62   48   48  -99]\n",
      "  [   0    0    0    0]\n",
      "  [  48  -99  -90  -90]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  32   21  -99  -90]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  54   42  -99  -99]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[ -99  -99   -6   17]\n",
      "  [ -99  -99   14   21]\n",
      "  [ -90  -99   -5   24]\n",
      "  [  -5  -99   21   28]\n",
      "  [ -99  -99   21   32]\n",
      "  [ -99   28   28   37]\n",
      "  [ -99  -99   32   42]\n",
      "  [  48  -99   37   37]\n",
      "  [ -99  -90   42   28]\n",
      "  [ -99  -99   37   -4]\n",
      "  [ -99  -90   32   -5]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  32   24  -99  -99]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [ -90  -90  -90   15]\n",
      "  [ -90  -90   -7   17]\n",
      "  [ -99   15   14   21]\n",
      "  [ -99  -99   17   24]\n",
      "  [  28  -99   21   21]\n",
      "  [ -99  -99   24   17]\n",
      "  [ -99   15   21   14]\n",
      "  [ -90  -99   17   -6]\n",
      "  [ -90  -99   15  -90]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  17   12  -99  -99]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [  17   12  -99  -99]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]]\n",
      "\n",
      " [[ -99  -99   -7    8]\n",
      "  [ -99  -99   -7   10]\n",
      "  [ -99  -99   -6   12]\n",
      "  [  15  -99   10    8]\n",
      "  [ -99  -99   12   -7]\n",
      "  [ -99  -90   10   -7]\n",
      "  [ -99  -90   -7   12]\n",
      "  [  15  -99   10   -7]\n",
      "  [ -90  -99   12   -7]\n",
      "  [ -99  -90   10   -7]\n",
      "  [ -90  -99    8    5]]\n",
      "\n",
      " [[   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]\n",
      "  [   0    0    0    0]]]\n"
     ]
    }
   ],
   "source": [
    "# display the final Q-values\n",
    "print('Q-values:\\n{}'.format(Q.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 9], [2, 9], [1, 9], [1, 8], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
      "[[5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n",
      "[[9, 5], [9, 4], [9, 3], [8, 3], [7, 3], [7, 4], [7, 5], [6, 5], [5, 5], [5, 6], [5, 7], [4, 7], [3, 7], [2, 7], [1, 7], [1, 6], [1, 5], [0, 5]]\n"
     ]
    }
   ],
   "source": [
    "# display a few shortest paths\n",
    "print(get_shortest_path(3, 9)) # row 3, col 9\n",
    "print(get_shortest_path(5, 0)) # row 5, col 0\n",
    "print(get_shortest_path(9, 5)) # row 9, col 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
